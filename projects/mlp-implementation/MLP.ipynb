{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRs8vJ6i8vWK"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<font color=#0000FF size=6>\n",
        " پیاده‌سازی MLP از پایه\n",
        "        </font>\n",
        "\t\t<hr>\n",
        "        <b>متن گزارش در محیط ژوپیتر محلی (local) به درستی نمایش داده می‌شود، ولی روی گوگل کولب مشکل دارد.</b>\n",
        "        <br>\n",
        "در ابتدا پیاده‌سازی الگوریتم مرتب‌سازی توپولوژیکی (Topological Sort) را قرار داده‌ایم چراکه در تعریف کلاس <code>Variable</code> به آن نیاز خواهیم داشت.\n",
        "        متغیر <code>var</code> که نماینده‌ی آخرین رأس گراف محاسباتی است به این الگوریتم داده شده و در نتیجه‌ی آن لیستی مرتب شده از رئوس گراف محاسباتی با فراخوانی متد <code>top_sort</code> خروجی داده می‌شود.\n",
        "        <br>\n",
        "        الگوریتم اصلی در متد بازگشتی <code>topsort_helper</code> آمده است.\n",
        "        این تابع یک رأس از گراف محاسباتی را دریافت کرده و بررسی می‌کند که آیا قبلاً آن را بازدید کرده است یا خیر.\n",
        "        چنانچه آن را بازدید نکرده باشد، در ادامه آن را بازدید کرده (یعنی به لیست مربوط به رئوس بازدید شده اضافه‌اش می‌کند) و اگر این رأس یکی از برگ‌های گراف باشد، آن را به لیست نهایی اضافه می‌کند.\n",
        "        در غیر این صورت به طور بازگشتی والدهای این رأس را مرتب‌سازی کرده و در نهایت خود رأس را هم به لیست نهایی اضافه می‌کند.\n",
        "        <br>\n",
        "        در اینجا از کلمه‌ی والد به جای فرزندان یک رأس استفاده می‌کنیم، چراکه در ساختار گراف محاسباتی که در ادامه آن را مورد بررسی قرار خواهیم داد، هر رأس غیر برگ از دو رأس دیگر ایجاد می‌شود و رأس ایجاد شده نشانگرهایی به رئوس والد خواهد داشت، نه رئوس والد به رأس ایجاد شده.\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn1dokpNzMeR"
      },
      "outputs": [],
      "source": [
        "class TopSort:\n",
        "    def __init__(self, var):\n",
        "        self.var = var\n",
        "\n",
        "    def topsort_helper(self, var):\n",
        "        if var not in self.seen:\n",
        "            self.seen.add(var)\n",
        "            if var.is_leaf():\n",
        "                self.sorted_variables.append(var)\n",
        "            else:\n",
        "                for parent in var.parents:\n",
        "                    self.topsort_helper(parent)\n",
        "                self.sorted_variables.append(var)\n",
        "\n",
        "    def top_sort(self):\n",
        "        self.seen = set()\n",
        "        self.sorted_variables = []\n",
        "        self.topsort_helper(self.var)\n",
        "        return self.sorted_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnEJheGY8vWR"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        در ادامه کلاس <code>Variable</code> را تعریف خواهیم کرد که نماینده‌ی رئوس گراف محاسباتی است.\n",
        "        هر شئ از این کلاس مقدار متغیر (<code>value</code>)، والدین متغیر (<code>parents</code>) و مشتق متغیری که روی آن متد <code>backward</code> فراخوانده می‌شود نسبت به خود این متغیر (<code>grad</code>) را در خود نگهداری می‌کند که مقدار اولیه‌ی آن هم برابر $0$ است و در هر بار محاسبه‌ی گرادیان باید مجدداً تنظیم شود.\n",
        "        <br>\n",
        "        متد <code>add_parent</code> در زمان ایجاد یک رأس جدید برای اینکه بتوانیم یال‌های گراف محاسباتی را تشکیل دهیم استفاده شده و در واقع والدهای رأس جدید را به لیست والدهای آن اضافه می‌کند.\n",
        "        <br>\n",
        "        چهار عملگر ضرب، جمع، تفریق و به توان رساندن برای این کلاس بازنویسی شده و در هر کدام رأس جدیدی که در خود نشانگرهایی به والد(های) خود دارند ایجاد می‌شود.\n",
        "        در روابط زیر $v_1$ متناظر با <code>self</code>، $v_2$ متناظر با <code>other</code> و $v_3$ متناظر با <code>new_variable</code> است و در نتیجه <code>v1.grad</code> مقدار $\\frac{\\partial v}{\\partial v_1}$، <code>v2.grad</code> مقدار $\\frac{\\partial v}{\\partial v_2}$ و <code>v3.grad</code> $\\frac{\\partial v}{\\partial v_3}$ را در خود ذخیره می‌کند که $v$ همان متغیری است که متد <code>backward</code> روی آن فراخوانی شده و می‌خواهیم گرادیانش را محاسبه کنیم.\n",
        "        <br>\n",
        "        در ضرب، متغیر جدید $v_3$ از ضرب دو متغیر قبلی $v_1$ و $v_2$ ایجاد می‌شود.\n",
        "        پس کلید <code>backward_</code> طبق رابطه‌ی زیر پیاده‌سازی شده است:\n",
        "        $$\n",
        "        \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial v_3}{\\partial v_1} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial (v_1 v_2)}{\\partial v_1} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} (v_2)\n",
        "        $$\n",
        "        و به طور مشابه:\n",
        "        $$\n",
        "        \\frac{\\partial v}{\\partial v_2} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial v_3}{\\partial v_2} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_2} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial (v_1 v_2)}{\\partial v_2} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_2} = \\frac{\\partial v}{\\partial v_3} (v_1)\n",
        "        $$\n",
        "        <br>\n",
        "        در جمع، متغیر جدید $v_3$ از جمع دو متغیر قبلی $v_1$ و $v_2$ ایجاد می‌شود و\n",
        "        لذا کلید <code>backward_</code> طبق رابطه‌ی زیر پیاده‌سازی شده است:\n",
        "        $$\n",
        "        \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial v_3}{\\partial v_1} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial (v_1 + v_2)}{\\partial v_1} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} (1)\n",
        "        $$\n",
        "        و به طور مشابه:\n",
        "        $$\n",
        "        \\frac{\\partial v}{\\partial v_2} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial v_3}{\\partial v_2} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_2} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial (v_1 + v_2)}{\\partial v_2} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_2} = \\frac{\\partial v}{\\partial v_3} (1)\n",
        "        $$\n",
        "        در تفریق، متغیر جدید $v_3$ از تفریق دو متغیر قبلی $v_1$ و $v_2$ ایجاد می‌شود و\n",
        "        بنابراین کلید <code>backward_</code> طبق رابطه‌ی زیر پیاده‌سازی شده است:\n",
        "        $$\n",
        "        \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial v_3}{\\partial v_1} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial (v_1 - v_2)}{\\partial v_1} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} (1)\n",
        "        $$\n",
        "        و به طور مشابه:\n",
        "        $$\n",
        "        \\frac{\\partial v}{\\partial v_2} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial v_3}{\\partial v_2} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_2} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial (v_1 - v_2)}{\\partial v_2} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_2} = \\frac{\\partial v}{\\partial v_3} (-1)\n",
        "        $$\n",
        "        در توان رساندن، متغیر جدید $v_3$ از به توان رساندن متغیر قبلی $v_1$ به توان عددی مثل $p$ ایجاد می‌شود و\n",
        "        لذا کلید <code>backward_</code> طبق رابطه‌ی زیر پیاده‌سازی شده است:\n",
        "        $$\n",
        "        \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial v_3}{\\partial v_1} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} \\frac{\\partial (v_1 ^ p)}{\\partial v_1} \\\\\n",
        "        \\Rightarrow \\frac{\\partial v}{\\partial v_1} = \\frac{\\partial v}{\\partial v_3} (p v_1 ^ {p - 1})\n",
        "        $$\n",
        "        متد <code>is_leaf</code> در صورتی که متغیر (در واقع رأس گراف محاسباتی) برگ باشد <code>True</code> و در غیر این صورت <code>False</code> برمی‌گرداند.\n",
        "        <br>\n",
        "        متد <code>backward</code> گرادیان متغیر ورودی را نسبت به تمامی متغیرها یا همان رئوس گراف محاسباتی به دست می‌آورد. به این صورت که ابتدا گرادیان خود متغیر را تنظیم می‌کند و آن را برابر $1$ قرار می‌دهد، چراکه $\\frac{\\partial v}{\\partial v} = 1$.\n",
        "        در ادامه یک بار مرتب‌سازی توپولوژیکی را با شروع از خود متغیر انجام داده و لیست خروجی را برعکس کرده و با پیمایش روی متغیرهای داخل لیست خروجی مرتب‌سازی شده، کلید <code>backward_</code> هر متغیر را صدا می‌زند.\n",
        "        <br>\n",
        "        چون از مسیرهای متفاوت می‌توان به یک رأس گراف رسید، در کلیدهای <code>backward_</code> داخل هر عملیات مشتق محاسبه شده را با مشتق قبلی جمع می‌کنیم.\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LJptnlhDsBV"
      },
      "outputs": [],
      "source": [
        "class Variable:\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.parents = []\n",
        "        self.grad = 0\n",
        "        self._backward = lambda : None\n",
        "\n",
        "    def add_parent(self, other_variable):\n",
        "        self.parents.append(other_variable)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Variable) else Variable(other)\n",
        "\n",
        "        new_variable = Variable(self.value * other.value)\n",
        "        new_variable.add_parent(self)\n",
        "        new_variable.add_parent(other)\n",
        "\n",
        "        def backward():\n",
        "            self.grad += new_variable.grad * other.value\n",
        "            other.grad += new_variable.grad * self.value\n",
        "\n",
        "        new_variable._backward = backward\n",
        "\n",
        "        return new_variable\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Variable) else Variable(other)\n",
        "\n",
        "        new_variable = Variable(self.value + other.value)\n",
        "        new_variable.add_parent(self)\n",
        "        new_variable.add_parent(other)\n",
        "\n",
        "        def backward():\n",
        "            self.grad += new_variable.grad * 1\n",
        "            other.grad += new_variable.grad * 1\n",
        "\n",
        "        new_variable._backward = backward\n",
        "\n",
        "        return new_variable\n",
        "\n",
        "    # this is added for `sum` function to work properly\n",
        "    def __radd__(self, other):\n",
        "        return self.__add__(other)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        other = other if isinstance(other, Variable) else Variable(other)\n",
        "\n",
        "        new_variable = Variable(self.value - other.value)\n",
        "        new_variable.add_parent(self)\n",
        "        new_variable.add_parent(other)\n",
        "\n",
        "        def backward():\n",
        "            self.grad += new_variable.grad * 1\n",
        "            other.grad += new_variable.grad * (-1)\n",
        "\n",
        "        new_variable._backward = backward\n",
        "\n",
        "        return new_variable\n",
        "\n",
        "    def __pow__(self, power):\n",
        "        new_variable = Variable(self.value ** power)\n",
        "        new_variable.add_parent(self)\n",
        "\n",
        "        def backward():\n",
        "            self.grad += new_variable.grad * power * (self.value ** (power - 1))\n",
        "\n",
        "        new_variable._backward = backward\n",
        "\n",
        "        return new_variable\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return not self.parents\n",
        "\n",
        "    def backward(self):\n",
        "        self.grad = 1\n",
        "        topsort = TopSort(self)\n",
        "        reverse_sorted_variables = topsort.top_sort()[::-1]\n",
        "        for var in reverse_sorted_variables:\n",
        "            var._backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S7_egh38vWV"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        در ادامه دو کلاس قبلی را مطابق <a href=\"https://w3.cs.jmu.edu/spragunr/CS445_S23/lectures/autodiff/autodiff.pdf\">این جزوه</a> بررسی کرده‌ایم.\n",
        "        <br>\n",
        "        در اینجا ابتدا متغیرها را تشکیل می‌دهیم:\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5Ht0IXGzHmT"
      },
      "outputs": [],
      "source": [
        "w = Variable(1)\n",
        "x = Variable(2)\n",
        "y = Variable(3)\n",
        "l0 = w * x\n",
        "l1 = l0 - y\n",
        "l2 = l1 ** 2\n",
        "l3 = w ** 2\n",
        "l4 = l2 + l3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfK6QIEo8vWX"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        حال الگوریتم مرتب‌سازی توپولوژیکی را فراخوانی کرده و مقدار هر متغیر را چاپ می‌کنیم که همان طور که مشاهده می‌شود مطابق ترتیبی است که در جزوه‌ی مذکور آمده است.\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ULNICm-TiQg",
        "outputId": "eba6e287-0f74-4b14-ff68-93a41ede307b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "2\n",
            "3\n",
            "-1\n",
            "1\n",
            "1\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "topsort = TopSort(l4)\n",
        "for var in topsort.top_sort():\n",
        "    print(var.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrgu6OIy8vWZ"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        حال متد <code>backward</code> را روی متغیر <code>l4</code> فراخوانی کرده و مشتقات جزئی هر متغیر را چاپ می‌کنیم که همگی صحیح و مطابق آنچه که در جزوه آمده است می‌باشد.\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGwY-aQSvLlN"
      },
      "outputs": [],
      "source": [
        "l4.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o9Rl4de1n81",
        "outputId": "ca9ae692-6722-4f92-e93b-97fafa9a1869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dl4/l4 = 1\n",
            "dl4/l3 = 1\n",
            "dl4/l2 = 1\n",
            "dl4/l1 = -2\n",
            "dl4/l0 = -2\n",
            "dl4/dw = -2\n",
            "dl4/dx = -2\n",
            "dl4/dy = 2\n"
          ]
        }
      ],
      "source": [
        "print(f'dl4/l4 = {l4.grad}')\n",
        "print(f'dl4/l3 = {l3.grad}')\n",
        "print(f'dl4/l2 = {l2.grad}')\n",
        "print(f'dl4/l1 = {l1.grad}')\n",
        "print(f'dl4/l0 = {l0.grad}')\n",
        "print(f'dl4/dw = {w.grad}')\n",
        "print(f'dl4/dx = {x.grad}')\n",
        "print(f'dl4/dy = {y.grad}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QQdOq_K8vWc"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        در ادامه کلاس <code>Neuron</code> که نماینده‌ی نورون‌های شبکه‌ی عصبی است را پیاده‌سازی کرده‌ایم.\n",
        "        این کلاس وزن‌ها و بایاس خود را به صورت پارامتر دریافت می‌کند و خروجی را به صورت زیر محاسبه می‌کند:\n",
        "        $$\n",
        "        z = \\sigma(w_0 + w_1 x_1 + w_2 x_n + ... + w_n x_n) = w_0 + w_1 x_1 + w_2 x_n + ... + w_n x_n\n",
        "        $$\n",
        "        که $w_0$ بایاس، $w_i$ها $(1 \\leq i \\leq n)$ وزن‌ها و $x_i$ها ورودی‌های نورون از لایه‌ی قبلی بوده و تابع فعال‌ساز هم خطی در نظر گرفته شده است.\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxnYX2GcrhKd"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    def __init__(self, weights):\n",
        "        self.weights = weights\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        z = Variable(self.weights[0].value)\n",
        "        for input_var, weight in zip(inputs, self.weights[1:]):\n",
        "            z += weight * input_var\n",
        "        return z\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-tQkdYM8vWd"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        در ادامه کلاس <code>Layer</code> که همان لایه‌های شبکه‌ی عصبی ما را تشکیل می‌دهد پیاده‌سازی شده است.\n",
        "        این کلاس با دریافت تعداد ورودی‌های لایه‌ی قبلی یعنی <code>input_size</code> و اندازه‌ی لایه که همان تعداد نورون‌های داخل لایه است (یعنی <code>layer_size</code>)، وزن‌ها و بایاس داخل هر نورون را با مقداردهی اولیه‌ی رندوم از بازه‌ی $(-1, 1)$ تشکیل داده و قادر است خروجی لایه را به صورت لیستی از خروجی‌های هر نورون لایه بازگرداند.\n",
        "        با این حال اگر خروجی تک متغیره بود، آن را به صورت یک متغیر و نه لیستی از یک متغیر بازمی‌گرداند.\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8feKlY1NWgk9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, layer_size):\n",
        "        self.neurons = []\n",
        "        for _ in range(layer_size):\n",
        "            weights = [Variable(random.uniform(-1, 1)) for _ in range(input_size + 1)]\n",
        "\n",
        "            self.neurons.append(Neuron(weights))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = [neuron(inputs) for neuron in self.neurons]\n",
        "        return outputs[0] if len(outputs) == 1 else outputs\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqTSP51f8vWe"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        کلاس <code>MLP</code> ورودی‌های شبکه و لیستی از تعداد نورون‌های هر لایه را دریافت کرده و شبکه را با وزن‌ها و بایاس اولیه تشکیل می‌دهد.\n",
        "        <br>\n",
        "        متد <code>parameters</code> با پیمایش روی کل لایه‌های شبکه و همچنین پیمایش روی نورون‌های هر لایه، وزن‌ها و بایاس هر نورون را به یک لیست اضافه کرده و در نهایت لیست نهایی را خروجی می‌دهد.\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiQ_zV3a_x9h"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, input_size, layer_sizes):\n",
        "        self.layers = []\n",
        "        # ip_size = input_size\n",
        "        for layer_size in layer_sizes:\n",
        "            self.layers.append(Layer(input_size, layer_size))\n",
        "            input_size = layer_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # outputs = None\n",
        "        # inputs = x\n",
        "        # for layer in self.layers:\n",
        "            # outputs = layer.get_outputs(inputs)\n",
        "            # inputs = outputs\n",
        "        # return outputs\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def parameters(self):\n",
        "        weights = []\n",
        "        for layer in self.layers:\n",
        "            for neuron in layer.neurons:\n",
        "                for weight in neuron.weights:\n",
        "                    weights.append(weight)\n",
        "        return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yss5Gvm8vWf"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        کلاس <code>Optimizer</code> پارامترهای مدل، یعنی تمام وزن‌ها و بایاس‌های آن را به همراه نرخ یادگیری <code>lr</code> یا همان learning rate دریافت کرده و از این طریق پارامترهای مدل را متناسباً بروزرسانی می‌کند.\n",
        "        <br>\n",
        "        متد <code>zero_grad</code> مشتق همه‌ی متغیرها را صفر کرده تا در هر ایپاک (epoch) مشتقات قبلی روی مشتقات جدید تأثیر نگذارند.\n",
        "        <br>\n",
        "        همچنین متد <code>step</code> به ازای هر پارامتر $w_i (0 \\leq i \\leq n)$ مقدار جدید را به صورت زیر به دست می‌آورد:\n",
        "        $$\n",
        "        w_i = w_i - \\eta \\frac{\\partial loss}{\\partial w_i}\n",
        "        $$\n",
        "        که $\\eta$ همان نرخ یادگیری و $loss$ تابع خطا می‌باشد.\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zwosm0MtR3hZ"
      },
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, parameters, lr):\n",
        "        self.parameters = parameters\n",
        "        self.lr = lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for parameter in self.parameters:\n",
        "            parameter.grad = 0\n",
        "\n",
        "    def step(self):\n",
        "        for parameter in self.parameters:\n",
        "            parameter.value -= self.lr * parameter.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVNq9zH28vWg"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        دیتاست فرضی را مطابق آنچه که در ویدیوی کارگاه گفته شد به صورت زیر تعریف می‌کنیم:\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJYVlC4wIVH9"
      },
      "outputs": [],
      "source": [
        "X = [\n",
        "    [2.0, 3.0, -1.0],\n",
        "    [3.0, -1.0, 0.5],\n",
        "    [0.5, 1.0, 1.0],\n",
        "    [1.0, 1.0, -1.0]\n",
        "]\n",
        "\n",
        "y = [1.0, -1.0, -1.0, 1.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWjfj2_58vWh"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        در ادامه مدلی فرضی مطابق ساختاری که در راهنما خواسته شده به صورت زیر ایجاد می‌کنیم:\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSPT0E-hqcsP"
      },
      "outputs": [],
      "source": [
        "input_size = 3\n",
        "layer_sizes = [2, 3, 1]\n",
        "model = MLP(input_size, layer_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTM6Smrw8vWh"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        یک <code>Optimizer</code> با نرخ یادگیری $0.01$ مطابق ویدیوی کارگاه ایجاد می‌کنیم:\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uAQQxlhKX9V"
      },
      "outputs": [],
      "source": [
        "optim = Optimizer(model.parameters(), 0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP7f4R8J8vWi"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        تابع <code>criterion</code> با دریافت دو بردار $\\hat{y}$ و $y$، تابع خطای MSE را به صورت زیر حساب کرده و به شکل یک <code>Variable</code> خروجی می‌دهد.\n",
        "        $$\n",
        "        loss = \\sum_{i=1}^n (\\hat{y_i} - y_i) ^ 2\n",
        "        $$\n",
        "        این تابع مطابق با ویدیوی کارگاه آورده شده است.\n",
        "        واضح است تقسیم بر $n$ تأثیر چندانی در کلیت الگوریتم ندارد، چرا که به صورت یک ثابت در رابطه ظاهر می‌شود.\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1EdMBGAEDvF"
      },
      "outputs": [],
      "source": [
        "def criterion(y_hats, y):\n",
        "    return sum([(y_hat - _y) ** 2 for y_hat, _y in zip(y_hats, y)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5685ZhK8vWi"
      },
      "source": [
        "<p></p>\n",
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<p></p>\n",
        "        در پایان الگوریتم یادگیری را به صورت زیر مطابق راهنما و ویدیوی کارگاه پیاده‌سازی کرده و مشاهده می‌کنیم که در هر گام مقدار تابع خطا کوچکتر می‌شود.\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc_EJHfpgJFr",
        "outputId": "8a1c0049-4f46-4e90-b9db-bd33687a9cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, loss: 4.552400449787093\n",
            "epoch: 2, loss: 3.4763025697606276\n",
            "epoch: 3, loss: 3.3217630691270528\n",
            "epoch: 4, loss: 3.1686015470939055\n",
            "epoch: 5, loss: 2.993653949789069\n",
            "epoch: 6, loss: 2.796027000052853\n",
            "epoch: 7, loss: 2.578528681901156\n",
            "epoch: 8, loss: 2.3472344739181805\n",
            "epoch: 9, loss: 2.111613145444208\n",
            "epoch: 10, loss: 1.8837361175374663\n",
            "epoch: 11, loss: 1.6762144484308275\n",
            "epoch: 12, loss: 1.499217768987232\n",
            "epoch: 13, loss: 1.3577814466979965\n",
            "epoch: 14, loss: 1.2508725665547584\n",
            "epoch: 15, loss: 1.1727662279565987\n",
            "epoch: 16, loss: 1.1157832027348333\n",
            "epoch: 17, loss: 1.0727709336227516\n",
            "epoch: 18, loss: 1.038360951710682\n",
            "epoch: 19, loss: 1.0090841751648687\n",
            "epoch: 20, loss: 0.9829228588113506\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 20\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # y_hats = []\n",
        "    # for x in X:\n",
        "    #     y_hats.append(model(x))\n",
        "    y_hats = [model(x) for x in X]\n",
        "\n",
        "    # loss = Variable(0)\n",
        "    # n = len(X)\n",
        "    # for j in range(n):\n",
        "    #     loss += (y_hats[j] - y[j]) ** 2\n",
        "    loss = criterion(y_hats, y)\n",
        "\n",
        "    optim.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    print(f'epoch: {epoch + 1}, loss: {loss.value}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}